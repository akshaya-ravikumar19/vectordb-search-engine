{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnD9WWo6YdZ/Ay3Bh6fyie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshaya-ravikumar19/vectordb-search-engine/blob/main/Using_FAISS_as_a_Vector_DB_for_Q_A_from_PDFs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers scikit-learn nltk transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Ensure NLTK tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# ==============================\n",
        "# PDF Text Extraction\n",
        "# ==============================\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "# ==============================\n",
        "# Text Chunking\n",
        "# ==============================\n",
        "def chunk_text(text, chunk_size=250, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "# ==============================\n",
        "# Ingest PDFs with Metadata\n",
        "# ==============================\n",
        "def ingest(pdf_folder):\n",
        "    records = []\n",
        "    pdf_folder = Path(pdf_folder)\n",
        "    for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        chunks = chunk_text(text)\n",
        "        for chunk in chunks:\n",
        "            if chunk.strip():\n",
        "                records.append((pdf_file.name, chunk.strip()))\n",
        "    return records\n",
        "\n",
        "# ==============================\n",
        "# Build FAISS Index\n",
        "# ==============================\n",
        "def build_faiss(records, index_file, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [r[1] for r in records]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_file)\n",
        "    return index, texts, records, model\n",
        "\n",
        "# ==============================\n",
        "# Smart Search with Year Filtering\n",
        "# ==============================\n",
        "def search(query, index, texts, model, records, top_k=3):\n",
        "    year_match = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "    query_year = year_match.group(1) if year_match else None\n",
        "\n",
        "    query_vec = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_vec, top_k * 5)  # fetch more, filter later\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        filename, chunk = records[idx]\n",
        "        if query_year and query_year not in filename:\n",
        "            continue\n",
        "        results.append((filename, chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    return results\n",
        "\n",
        "# ==============================\n",
        "# Reader (QA or Summarizer)\n",
        "# ==============================\n",
        "# Option 1: Question Answering (extracts exact span)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "# Option 2: Summarization (more natural full sentence)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question, top_chunks, method=\"qa\"):\n",
        "    context = \" \".join(chunk for _, chunk in top_chunks)\n",
        "    context = context[:1500]  # avoid token overflow\n",
        "\n",
        "    if method == \"qa\":\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return f\"Q: {question}\\nA: {result['answer']}\"\n",
        "    else:\n",
        "        summary = summarizer(context, max_length=200, min_length=50, do_sample=False)\n",
        "        return f\"Q: {question}\\nA: {summary[0]['summary_text']}\"\n",
        "\n",
        "# ==============================\n",
        "# Run Pipeline\n",
        "# ==============================\n",
        "pdf_folder = Path(\"/content/sample_data/ESG_Files\")  # update path\n",
        "index_file = \"esg_index.faiss\"\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"Building indices from: {pdf_folder.resolve()}\")\n",
        "records = ingest(pdf_folder)\n",
        "index, texts, records, model = build_faiss(records, index_file, model_name)\n",
        "print(\"Index built successfully!\")\n",
        "\n",
        "# ==============================\n",
        "# Example Queries\n",
        "# ==============================\n",
        "questions = [\n",
        "    \"What sustainability initiatives did Walmart report in 2020?\",\n",
        "    \"What were the key ESG targets for Amazon in 2023?\",\n",
        "    \"How is Walmart managing ESG risks and compliance?\",\n",
        "    \"What carbon reduction initiatives did Amazon implement in 2021?\"\n",
        "  ]\n",
        "\n",
        "for q in questions:\n",
        "    chunks = search(q, index, texts, model, records, top_k=5)\n",
        "    answer = generate_answer(q, chunks, method=\"summarizer\")  # switch to \"qa\" if you prefer exact spans\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d004jE0S0ss",
        "outputId": "3f564051-dd6c-45b7-80e8-53012ea09626"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indices from: /content/sample_data/ESG_Files\n",
            "Processing amazon_sustainability_report_2020.pdf ...\n",
            "Processing walmart_esgreport_2022.pdf ...\n",
            "Processing amazon_sustainability_report_2023.pdf ...\n",
            "Processing walmart_esgreport_2023.pdf ...\n",
            "Processing amazon_sustainability_report_2024.pdf ...\n",
            "Processing walmart_esgreport_2020.pdf ...\n",
            "Processing walmart_esgreport_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2022 (Executive Summary).pdf ...\n",
            "Index built successfully!\n",
            "\n",
            "================================================================================\n",
            "Q: What sustainability initiatives did Walmart report in 2020?\n",
            "A: More than 2,300 suppliers have signed on to Project GigatonTM. Since 2017, suppliers have reported a cumulative 230 million metric tons of avoided emissions. Walmart provided cash and in-kind donations of more than $1.4 billion to projects that create opportunity.\n",
            "\n",
            "================================================================================\n",
            "Q: What were the key ESG targets for Amazon in 2023?\n",
            "A: Amazon’s first priority under The Climate Pledge is to eliminate emissions within the value chain of our businesses. In parallel, we are investing in climate mitigation outside of our value chain (“carbon neutralization”) We plan to neutralize any emissions that cannot be eliminated by 2040.\n",
            "\n",
            "================================================================================\n",
            "Q: How is Walmart managing ESG risks and compliance?\n",
            "A: Every Walmart associate is responsible for upholding our high ethical standards and complying with all relevant laws and regulations. Our compliance expectations apply to all Walmart associates, suppliers and contractors and extend to the highest levels of the company. The Audit Committee of our Board of Directors has risk oversig\n",
            "\n",
            "================================================================================\n",
            "Q: What carbon reduction initiatives did Amazon implement in 2021?\n",
            "A: Reach net-zero carbon emissions across our operations by 2040 Reduce food waste by 50% across our U.S. and Europe operations by 2030. Make 50% of Amazon shipments net- zero carbon by 2030 Renewable Energy With 274 renewable projects announced as of the end of 2021, Amazon is the world’s largest corporate purchaser of renewable energy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers scikit-learn nltk transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Ensure NLTK tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "#PDF Text Extraction\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "#Text Chunking\n",
        "def chunk_text(text, chunk_size=250, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "#Ingest PDFs with Metadata\n",
        "def ingest(pdf_folder):\n",
        "    records = []\n",
        "    pdf_folder = Path(pdf_folder)\n",
        "    for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        chunks = chunk_text(text)\n",
        "        for chunk in chunks:\n",
        "            if chunk.strip():\n",
        "                records.append((pdf_file.name, chunk.strip()))\n",
        "    return records\n",
        "\n",
        "#Build FAISS Index\n",
        "def build_faiss(records, index_file, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [r[1] for r in records]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_file)\n",
        "    return index, texts, records, model\n",
        "\n",
        "#Smart Search with Year Filtering\n",
        "def search(query, index, texts, model, records, top_k=3):\n",
        "    year_match = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "    query_year = year_match.group(1) if year_match else None\n",
        "\n",
        "    query_vec = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_vec, top_k * 5)  # fetch more, filter later\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        filename, chunk = records[idx]\n",
        "        if query_year and query_year not in filename:\n",
        "            continue\n",
        "        results.append((filename, chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    return results\n",
        "\n",
        "#Reader (QA or Summarizer)\n",
        "#Option 1: Question Answering (extracts exact span)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "#Option 2: Summarization (more natural full sentence)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question, top_chunks, method=\"qa\"):\n",
        "    context = \" \".join(chunk for _, chunk in top_chunks)\n",
        "    context = context[:1500]  # avoid token overflow\n",
        "\n",
        "    if method == \"qa\":\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return f\"Q: {question}\\nA: {result['answer']}\"\n",
        "    else:\n",
        "        summary = summarizer(context, max_length=200, min_length=50, do_sample=False)\n",
        "        return f\"Q: {question}\\nA: {summary[0]['summary_text']}\"\n",
        "\n",
        "#Run Pipeline\n",
        "pdf_folder = Path(\"/content/sample_data/ESG_Files\")  # update path\n",
        "index_file = \"esg_index.faiss\"\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"Building indices from: {pdf_folder.resolve()}\")\n",
        "records = ingest(pdf_folder)\n",
        "index, texts, records, model = build_faiss(records, index_file, model_name)\n",
        "print(\"Index built successfully!\")\n",
        "\n",
        "#Queries\n",
        "questions = [\n",
        "    \"What sustainability initiatives did Walmart report in 2020?\",\n",
        "    \"What were the key ESG targets for Amazon in 2023?\",\n",
        "    \"How is Walmart managing ESG risks and compliance?\",\n",
        "    \"What carbon reduction initiatives did Amazon implement in 2021?\"\n",
        "  ]\n",
        "\n",
        "for q in questions:\n",
        "    chunks = search(q, index, texts, model, records, top_k=5)\n",
        "    answer = generate_answer(q, chunks, method=\"qa\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX9pRrDlW7_9",
        "outputId": "2ef1afaa-a6f4-4ec4-ea25-42b1aa902415"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indices from: /content/sample_data/ESG_Files\n",
            "Processing amazon_sustainability_report_2020.pdf ...\n",
            "Processing walmart_esgreport_2022.pdf ...\n",
            "Processing amazon_sustainability_report_2023.pdf ...\n",
            "Processing walmart_esgreport_2023.pdf ...\n",
            "Processing amazon_sustainability_report_2024.pdf ...\n",
            "Processing walmart_esgreport_2020.pdf ...\n",
            "Processing walmart_esgreport_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2022 (Executive Summary).pdf ...\n",
            "Index built successfully!\n",
            "\n",
            "================================================================================\n",
            "Q: What sustainability initiatives did Walmart report in 2020?\n",
            "A: climate, waste\n",
            "\n",
            "================================================================================\n",
            "Q: What were the key ESG targets for Amazon in 2023?\n",
            "A: eliminate emissions within the value chain of our businesses\n",
            "\n",
            "================================================================================\n",
            "Q: How is Walmart managing ESG risks and compliance?\n",
            "A: Team members serve as subject matter experts and advisors on critical topics\n",
            "\n",
            "================================================================================\n",
            "Q: What carbon reduction initiatives did Amazon implement in 2021?\n",
            "A: 274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers scikit-learn nltk transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Ensure NLTK tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "#PDF Text Extraction\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "#Text Chunking\n",
        "def chunk_text(text, chunk_size=250, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "#Ingest PDFs with Metadata\n",
        "def ingest(pdf_folder):\n",
        "    records = []\n",
        "    pdf_folder = Path(pdf_folder)\n",
        "    for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        chunks = chunk_text(text)\n",
        "        for chunk in chunks:\n",
        "            if chunk.strip():\n",
        "                records.append((pdf_file.name, chunk.strip()))\n",
        "    return records\n",
        "\n",
        "#Build FAISS Index\n",
        "def build_faiss(records, index_file, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [r[1] for r in records]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_file)\n",
        "    return index, texts, records, model\n",
        "\n",
        "#Smart Search with Year Filtering\n",
        "def search(query, index, texts, model, records, top_k=3):\n",
        "    year_match = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "    query_year = year_match.group(1) if year_match else None\n",
        "\n",
        "    query_vec = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_vec, top_k * 5)  # fetch more, filter later\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        filename, chunk = records[idx]\n",
        "        if query_year and query_year not in filename:\n",
        "            continue\n",
        "        results.append((filename, chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    return results\n",
        "\n",
        "#Reader (QA or Summarizer)\n",
        "#Option 1: Question Answering (extracts exact span)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "#Option 2: Summarization (more natural full sentence)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question, top_chunks, method=\"qa\"):\n",
        "    context = \" \".join(chunk for _, chunk in top_chunks)\n",
        "    context = context[:1500]  # avoid token overflow\n",
        "\n",
        "    if method == \"qa\":\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return f\"Q: {question}\\nA: {result['answer']}\"\n",
        "    else:\n",
        "        summary = summarizer(context, max_length=200, min_length=50, do_sample=False)\n",
        "        return f\"Q: {question}\\nA: {summary[0]['summary_text']}\"\n",
        "\n",
        "#Run Pipeline\n",
        "pdf_folder = Path(\"/content/sample_data/ESG_Files\")  # update path\n",
        "index_file = \"esg_index.faiss\"\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"Building indices from: {pdf_folder.resolve()}\")\n",
        "records = ingest(pdf_folder)\n",
        "index, texts, records, model = build_faiss(records, index_file, model_name)\n",
        "print(\"Index built successfully!\")\n",
        "\n",
        "#Queries\n",
        "questions = [\n",
        "'What is the total Scope 1 emissions?',\n",
        "'What is the total Scope 2 emissions?',\n",
        "'What is the total Scope 3 emissions?',\n",
        "'Are science-based targets disclosed?',\n",
        "'Has the company committed to net-zero?',\n",
        "'What percentage of energy is renewable?',\n",
        "'Is energy efficiency improving year over year?',\n",
        "'What is the total water withdrawal?',\n",
        "'What is water recycled or reused?',\n",
        "'Is the company exposed to water stress?',\n",
        "'How much total waste is generated?',\n",
        "'How much waste is recycled or diverted from landfill?',\n",
        "'Are hazardous waste levels disclosed?',\n",
        "'How sustainable are raw material sourcing practices?',\n",
        "'Are biodiversity risks addressed?',\n",
        "  ]\n",
        "\n",
        "for q in questions:\n",
        "    chunks = search(q, index, texts, model, records, top_k=5)\n",
        "    answer = generate_answer(q, chunks, method=\"qa\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GzZ83znsKUf",
        "outputId": "1116586d-5041-43ca-f563-a833b5669be7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indices from: /content/sample_data/ESG_Files\n",
            "Processing amazon_sustainability_report_2020.pdf ...\n",
            "Processing walmart_esgreport_2022.pdf ...\n",
            "Processing amazon_sustainability_report_2023.pdf ...\n",
            "Processing walmart_esgreport_2023.pdf ...\n",
            "Processing amazon_sustainability_report_2024.pdf ...\n",
            "Processing walmart_esgreport_2020.pdf ...\n",
            "Processing walmart_esgreport_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2022 (Executive Summary).pdf ...\n",
            "Index built successfully!\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 1 emissions?\n",
            "A: Direct Emissions\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 2 emissions?\n",
            "A: 15.6%\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 3 emissions?\n",
            "A: 15.6%\n",
            "\n",
            "================================================================================\n",
            "Q: Are science-based targets disclosed?\n",
            "A: working toward setting science-based targets\n",
            "\n",
            "================================================================================\n",
            "Q: Has the company committed to net-zero?\n",
            "A: the path to achieving net- zero carbon will be challenging\n",
            "\n",
            "================================================================================\n",
            "Q: What percentage of energy is renewable?\n",
            "A: 7.7%\n",
            "\n",
            "================================================================================\n",
            "Q: Is energy efficiency improving year over year?\n",
            "A: 2015\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total water withdrawal?\n",
            "A: 41% in 2023\n",
            "\n",
            "================================================================================\n",
            "Q: What is water recycled or reused?\n",
            "A: recycling\n",
            "\n",
            "================================================================================\n",
            "Q: Is the company exposed to water stress?\n",
            "A: Acute A warming climate could increase thermal stress\n",
            "\n",
            "================================================================================\n",
            "Q: How much total waste is generated?\n",
            "A: 1.4 million\n",
            "\n",
            "================================================================================\n",
            "Q: How much waste is recycled or diverted from landfill?\n",
            "A: Percentage\n",
            "\n",
            "================================================================================\n",
            "Q: Are hazardous waste levels disclosed?\n",
            "A: may be ineligible for recovery pathways for health and safety reasons\n",
            "\n",
            "================================================================================\n",
            "Q: How sustainable are raw material sourcing practices?\n",
            "A: responsibly\n",
            "\n",
            "================================================================================\n",
            "Q: Are biodiversity risks addressed?\n",
            "A: Amazon’s approach to biodiversity follows a mitigation hierarchy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers scikit-learn nltk transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Ensure NLTK tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "#PDF Text Extraction\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "#Text Chunking\n",
        "def chunk_text(text, chunk_size=250, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "#Ingest PDFs with Metadata\n",
        "def ingest(pdf_folder):\n",
        "    records = []\n",
        "    pdf_folder = Path(pdf_folder)\n",
        "    for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        chunks = chunk_text(text)\n",
        "        for chunk in chunks:\n",
        "            if chunk.strip():\n",
        "                records.append((pdf_file.name, chunk.strip()))\n",
        "    return records\n",
        "\n",
        "#Build FAISS Index\n",
        "def build_faiss(records, index_file, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [r[1] for r in records]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_file)\n",
        "    return index, texts, records, model\n",
        "\n",
        "#Smart Search with Year Filtering\n",
        "def search(query, index, texts, model, records, top_k=3):\n",
        "    year_match = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "    query_year = year_match.group(1) if year_match else None\n",
        "\n",
        "    query_vec = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_vec, top_k * 5)  # fetch more, filter later\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        filename, chunk = records[idx]\n",
        "        if query_year and query_year not in filename:\n",
        "            continue\n",
        "        results.append((filename, chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    return results\n",
        "\n",
        "#Reader (QA or Summarizer)\n",
        "#Option 1: Question Answering (extracts exact span)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "#Option 2: Summarization (more natural full sentence)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question, top_chunks, method=\"qa\"):\n",
        "    context = \" \".join(chunk for _, chunk in top_chunks)\n",
        "    context = context[:1500]  # avoid token overflow\n",
        "\n",
        "    if method == \"qa\":\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return f\"Q: {question}\\nA: {result['answer']}\"\n",
        "    else:\n",
        "        summary = summarizer(context, max_length=200, min_length=50, do_sample=False)\n",
        "        return f\"Q: {question}\\nA: {summary[0]['summary_text']}\"\n",
        "\n",
        "#Run Pipeline\n",
        "pdf_folder = Path(\"/content/sample_data/ESG_Files\")\n",
        "index_file = \"esg_index.faiss\"\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"Building indices from: {pdf_folder.resolve()}\")\n",
        "records = ingest(pdf_folder)\n",
        "index, texts, records, model = build_faiss(records, index_file, model_name)\n",
        "print(\"Index built successfully!\")\n",
        "\n",
        "#Queries\n",
        "questions = [\n",
        "'What is the total Scope 1 emissions?',\n",
        "'What is the total Scope 2 emissions?',\n",
        "'What is the total Scope 3 emissions?',\n",
        "'Are science-based targets disclosed?',\n",
        "'Has the company committed to net-zero?',\n",
        "'What percentage of energy is renewable?',\n",
        "'Is energy efficiency improving year over year?',\n",
        "'What is the total water withdrawal?',\n",
        "'What is water recycled or reused?',\n",
        "'Is the company exposed to water stress?',\n",
        "'How much total waste is generated?',\n",
        "'How much waste is recycled or diverted from landfill?',\n",
        "'Are hazardous waste levels disclosed?',\n",
        "'How sustainable are raw material sourcing practices?',\n",
        "'Are biodiversity risks addressed?',\n",
        "  ]\n",
        "\n",
        "for q in questions:\n",
        "    chunks = search(q, index, texts, model, records, top_k=5)\n",
        "    answer = generate_answer(q, chunks, method=\"summarizer\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9YvA2YrtIQ_",
        "outputId": "265dc21c-ff37-4bc6-a59a-914a656b07a6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indices from: /content/sample_data/ESG_Files\n",
            "Processing amazon_sustainability_report_2020.pdf ...\n",
            "Processing walmart_esgreport_2022.pdf ...\n",
            "Processing amazon_sustainability_report_2023.pdf ...\n",
            "Processing walmart_esgreport_2023.pdf ...\n",
            "Processing amazon_sustainability_report_2024.pdf ...\n",
            "Processing walmart_esgreport_2020.pdf ...\n",
            "Processing walmart_esgreport_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2022 (Executive Summary).pdf ...\n",
            "Index built successfully!\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 1 emissions?\n",
            "A: The GHG Protocol outlines three emissions sources (referred to as “scopes”) that provide the framework for operational boundaries. The three scopes are: Scope 1, “Direct Emissions,” represent emissions from the combustible fuels and other sources that occur directly on sites. Scope 2, ‘Indirect Emissions’ represent emissions that occur off-site to produce electricity or steam.\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 2 emissions?\n",
            "A: The GHG Protocol outlines three emissions sources (referred to as “scopes”) that provide the framework for operational boundaries. Scope 1, “Direct Emissions,” represent emissions from the combustible fuels and other sources that occur directly on sites. Scope 2, ‘Indirect Emissions’ represent emissions that occur off-site to produce electricity or steam purchased for use at corporate locations. Scope 3, 'Other Indirect' represents emissions from activities down or upstream from a company’s core business.\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total Scope 3 emissions?\n",
            "A: The GHG Protocol outlines three emissions sources (referred to as “scopes”) that provide the framework for operational boundaries. Scope 1, “Direct Emissions,” represent emissions from the combustible fuels and other sources that occur directly on sites. Scope 2, ‘Indirect Emissions’ represent emissions that occur off-site to produce electricity or steam purchased for use at corporate locations. Scope 3, 'Other Indirect' represents emissions from activities down or upstream from a company’s core business.\n",
            "\n",
            "================================================================================\n",
            "Q: Are science-based targets disclosed?\n",
            "A: Amazon co-founded The Climate Pledge. We have a goal to deliver 50% of Amazon shipments with net-zero carbon by 2030. We are also on a path to powering our operations with 100% renewable energy by 2025. See our public methodology for more on our approach.\n",
            "\n",
            "================================================================================\n",
            "Q: Has the company committed to net-zero?\n",
            "A: Environment is the world’s largest corporate purchaser of renewable energy and reached 65% renewable energy across our business. Delivered more than 20 million packages to customers in electric delivery vehicles across North America and Europe in 2020. Secured the naming rights to Climate Pledge Arena and submitted registration to become the first net-zero carbon certified arena.\n",
            "\n",
            "================================================================================\n",
            "Q: What percentage of energy is renewable?\n",
            "A: This report includes generation from more than 530 active or under development renewable and low ‑carbon projects across eight countries, 26 U.S. states and Puerto Rico. On an adjusted basis, between 2015 baseline and 2018, Walmart reduced its absolute Scope 1 and 2 emissions by 7.7%, equivalent to 1.5 million metric tons of CO2e.\n",
            "\n",
            "================================================================================\n",
            "Q: Is energy efficiency improving year over year?\n",
            "A: Goal: Achieve an 18% emissions reduction in Walmart’s operations by 2025 (over 2015 baseline) Goal: Power 50% of our operations with renewable sources of energy by 2025. Goal: Drive the production or procurement of 7 billion kilowatt hours (kWh) of renewable energy globally by Dec. 31, 2020.\n",
            "\n",
            "================================================================================\n",
            "Q: What is the total water withdrawal?\n",
            "A: AWS announced its commitment to being water positive by 2030. To meet this goal, AWS is focused on improving liters per kilowatt-hour (L/kWh) water use effectiveness. Amazon seeks to minimize water use across our global data centers.\n",
            "\n",
            "================================================================================\n",
            "Q: What is water recycled or reused?\n",
            "A: Our systems are designed to separate and consolidate materials into their respective collection locations. We aim to select service providers that prioritize recycling over landfill and incineration. In 2024, we collaborated with UPM Raflatac’s RafCycle service to launch a recycling solution for the paper backing from our adhesive labels.\n",
            "\n",
            "================================================================================\n",
            "Q: Is the company exposed to water stress?\n",
            "A: Climate-related events could impact our ability to access raw materials and deliver final products. Repeated damage due to flooding could result in an inability to insure existing buildings. Warming climate could increase thermal stress and outdoor associate exposure to criteria air pollutants. Increasing regulations and evolving public preference may require a faster transition to a low-carbon economy.\n",
            "\n",
            "================================================================================\n",
            "Q: How much total waste is generated?\n",
            "A: Waste diversion: Percentage of waste materials diverted from landfill and incineration globally: 80%4 (CY2019) Goal: Achieve zero waste to landfill from our operations in the U.S., U.K., Japan, and Canada by 2025 in accordance with Zero Waste International Alliance guidelines. Food donations: Pounds of food donated globally >680 million pounds.\n",
            "\n",
            "================================================================================\n",
            "Q: How much waste is recycled or diverted from landfill?\n",
            "A: Waste diversion: Percentage of waste materials diverted from landfill and incineration globally: 80%4 (CY2019) Goal: Achieve zero waste to landfill from our operations in the U.S., U.K., Japan, and Canada by 2025 in accordance with Zero Waste International Alliance guidelines. Food donations: Pounds of food donated globally >680 million pounds.\n",
            "\n",
            "================================================================================\n",
            "Q: Are hazardous waste levels disclosed?\n",
            "A: Ships in Product Packaging was formerly called Ships in Own Container. Amazon’s 2022 total plastic packaging use has been updated to 86,055 metric tons. Being water positive means AWS will return more water to communities and the environment.\n",
            "\n",
            "================================================================================\n",
            "Q: How sustainable are raw material sourcing practices?\n",
            "A: We aim to reduce the emissions and waste associated with the design, development, and delivery of Amazon- and Whole Foods Market-branded products. This entails sourcing minerals, commodities, and agricultural products responsibly; using safer chemicals; and incorporating recycled materials where possible. We have also established goals and ambitions related to more sustainable seafood and animal welfare.\n",
            "\n",
            "================================================================================\n",
            "Q: Are biodiversity risks addressed?\n",
            "A: Biodiversity is declining globally due to land use change, direct exploitation, climate change, pollution, and the proliferation of invasive species. Since 1970, global wildlife populations have dropped by an average of 73%. Amazon’s approach to biodiversity follows a mitigation hierarchy that prioritizes avoidance and reduction of habitat loss above other actions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2 faiss-cpu sentence-transformers scikit-learn nltk transformers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import faiss\n",
        "from pathlib import Path\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "\n",
        "#Ensure NLTK tokenizers\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "#PDF Text Extraction\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        reader = PdfReader(pdf_path)\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "#Text Chunking\n",
        "def chunk_text(text, chunk_size=250, overlap=50):\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    for i in range(0, len(words), chunk_size - overlap):\n",
        "        chunk = \" \".join(words[i:i+chunk_size])\n",
        "        chunks.append(chunk)\n",
        "    return chunks\n",
        "\n",
        "#Ingest PDFs with Metadata\n",
        "def ingest(pdf_folder):\n",
        "    records = []\n",
        "    pdf_folder = Path(pdf_folder)\n",
        "    for pdf_file in pdf_folder.glob(\"*.pdf\"):\n",
        "        print(f\"Processing {pdf_file.name} ...\")\n",
        "        text = extract_text_from_pdf(pdf_file)\n",
        "        chunks = chunk_text(text)\n",
        "        for chunk in chunks:\n",
        "            if chunk.strip():\n",
        "                records.append((pdf_file.name, chunk.strip()))\n",
        "    return records\n",
        "\n",
        "#Build FAISS Index\n",
        "def build_faiss(records, index_file, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    model = SentenceTransformer(model_name)\n",
        "    texts = [r[1] for r in records]\n",
        "    embeddings = model.encode(texts, convert_to_numpy=True)\n",
        "\n",
        "    dim = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, index_file)\n",
        "    return index, texts, records, model\n",
        "\n",
        "#Smart Search with Year Filtering\n",
        "def search(query, index, texts, model, records, top_k=3):\n",
        "    year_match = re.search(r\"\\b(20\\d{2})\\b\", query)\n",
        "    query_year = year_match.group(1) if year_match else None\n",
        "\n",
        "    query_vec = model.encode([query], convert_to_numpy=True)\n",
        "    D, I = index.search(query_vec, top_k * 5)  # fetch more, filter later\n",
        "\n",
        "    results = []\n",
        "    for idx in I[0]:\n",
        "        filename, chunk = records[idx]\n",
        "        if query_year and query_year not in filename:\n",
        "            continue\n",
        "        results.append((filename, chunk))\n",
        "        if len(results) >= top_k:\n",
        "            break\n",
        "    return results\n",
        "\n",
        "#Reader (QA or Summarizer)\n",
        "#Option 1: Question Answering (extracts exact span)\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "#Option 2: Summarization (more natural full sentence)\n",
        "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
        "\n",
        "def generate_answer(question, top_chunks, method=\"qa\"):\n",
        "    context = \" \".join(chunk for _, chunk in top_chunks)\n",
        "    context = context[:1500]  # avoid token overflow\n",
        "\n",
        "    if method == \"qa\":\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return f\"Q: {question}\\nA: {result['answer']}\"\n",
        "    else:\n",
        "        summary = summarizer(context, max_length=200, min_length=50, do_sample=False)\n",
        "        return f\"Q: {question}\\nA: {summary[0]['summary_text']}\"\n",
        "\n",
        "#Run Pipeline\n",
        "pdf_folder = Path(\"/content/sample_data/ESG_Files\")\n",
        "index_file = \"esg_index.faiss\"\n",
        "model_name = \"all-MiniLM-L6-v2\"\n",
        "\n",
        "print(f\"Building indices from: {pdf_folder.resolve()}\")\n",
        "records = ingest(pdf_folder)\n",
        "index, texts, records, model = build_faiss(records, index_file, model_name)\n",
        "print(\"Index built successfully!\")\n",
        "\n",
        "#Queries\n",
        "questions = [\n",
        "      \"How have Amazon’s sustainability initiatives improved over the past 3 years?\",\n",
        "      \"What are the key ESG highlights from the Walmart's latest esg report?\",\n",
        "      \"Which ESG goals were achieved and which ones are pending overall by Walmart over the past 4 years?\",\n",
        "      \"How does Amazon compare to industry standards in ESG performance?\",\n",
        "      \"What challenges did Walmart report in implementing ESG strategies in 2023?\"\n",
        "  ]\n",
        "\n",
        "for q in questions:\n",
        "    chunks = search(q, index, texts, model, records, top_k=5)\n",
        "    answer = generate_answer(q, chunks, method=\"summarizer\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swuTFEQlGmJl",
        "outputId": "3a24ae25-8950-4d91-b612-cdb9ed4910b5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.35.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Device set to use cpu\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building indices from: /content/sample_data/ESG_Files\n",
            "Processing amazon_sustainability_report_2020.pdf ...\n",
            "Processing walmart_esgreport_2022.pdf ...\n",
            "Processing amazon_sustainability_report_2023.pdf ...\n",
            "Processing walmart_esgreport_2023.pdf ...\n",
            "Processing amazon_sustainability_report_2024.pdf ...\n",
            "Processing walmart_esgreport_2020.pdf ...\n",
            "Processing walmart_esgreport_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2021.pdf ...\n",
            "Processing amazon_sustainability_report_2022 (Executive Summary).pdf ...\n",
            "Index built successfully!\n",
            "\n",
            "================================================================================\n",
            "Q: How have Amazon’s sustainability initiatives improved over the past 3 years?\n",
            "A: Amazon has a presence in communities throughout the world. offer competitive pay and benefits, upskilling and educational development programs, and a workplace that promotes inclusion and diversity. We seek to be a good neighbor wherever we operate and to support local people and charitable organizations that meet on-the-ground needs.\n",
            "\n",
            "================================================================================\n",
            "Q: What are the key ESG highlights from the Walmart's latest esg report?\n",
            "A: The Annual Summary covers our activities during the fiscal year ending January 31, 2022. Beginning in 2021, our reporting has been centered around a series of ESG issue briefs covering each of Walmart’s priority ESG issues in depth. All references to “Walmart” in our ESG reporting are to Walmart Inc., a Delaware corporation.\n",
            "\n",
            "================================================================================\n",
            "Q: Which ESG goals were achieved and which ones are pending overall by Walmart over the past 4 years?\n",
            "A: The Annual Summary covers our activities during the fiscal year ending January 31, 2022. Beginning in 2021, our reporting has been centered around a series of ESG issue briefs covering each of Walmart’s priority ESG issues in depth. All references to “Walmart” in our ESG reporting are to Walmart Inc., a Delaware corporation.\n",
            "\n",
            "================================================================================\n",
            "Q: How does Amazon compare to industry standards in ESG performance?\n",
            "A: Induced spend denotes spend generated in communities of suppliers’ employees. This measures the purchases through these employees and jobs supported through these purchases. Certified Tier 2 diverse businesses are businesses that provide goods and services to Amazon’s Tier 1 suppliers. Affordable multifamily housing data is calculated through March 2024.\n",
            "\n",
            "================================================================================\n",
            "Q: What challenges did Walmart report in implementing ESG strategies in 2023?\n",
            "A: Shared value lies at the heart of Walmart’s enterprise strategy and our approach to ESG issues. We believe we maximize long-term value for shareholders by serving our stakeholders. We aspire to become a regenerative company — helping to renew people and the planet through our business.\n"
          ]
        }
      ]
    }
  ]
}